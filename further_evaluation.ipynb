{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "735da6b6",
   "metadata": {},
   "source": [
    "# Evaluation Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c41e6114",
   "metadata": {},
   "source": [
    "#### Uninteresting stuff we already talked about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f11c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from imblearn import over_sampling, under_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "maintenance = pd.read_csv(\"predictive_maintenance_prepared.csv\")\n",
    "\n",
    "X = maintenance[maintenance.columns[0:-1]]\n",
    "y = maintenance['Target']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=20, test_size=0.3, stratify=y)\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4d2e4ea",
   "metadata": {},
   "source": [
    "## Predicting via RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0ba630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.23%\n",
      "Recall: 52.94%\n",
      "Precision: 91.53%\n",
      "F1 Score: 67.08%\n"
     ]
    }
   ],
   "source": [
    "rf_model.fit(train_X, train_y)\n",
    "pred_y = rf_model.predict(test_X)\n",
    "accuracy_score = metrics.accuracy_score(pred_y, test_y)\n",
    "\n",
    "print('Accuracy: {:.2%}'.format(metrics.accuracy_score(test_y, pred_y)))\n",
    "print('Recall: {:.2%}'.format(metrics.recall_score(test_y, pred_y)))\n",
    "print('Precision: {:.2%}'.format(metrics.precision_score(test_y, pred_y)))\n",
    "print('F1 Score: {:.2%}'.format(metrics.f1_score(test_y, pred_y)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb59cdf8",
   "metadata": {},
   "source": [
    "We can see that even tho the Accuracy is quite high, the Recall and F1 score are considerably low. As we work with machines a false negative would be detrimental."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c477f609",
   "metadata": {},
   "source": [
    "## Random Oversampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f05b4eb",
   "metadata": {},
   "source": [
    "As we could see in the example before, the amount of faulty machines is way smaller than the amount of working machines. To fix this, first we are going to try to oversample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd75a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9c47ae7",
   "metadata": {},
   "source": [
    "## Over- and undersampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "63c444f2625e787ea2a4007195cd95f0ab18b1bf75026f8748d6fc196560cdf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
