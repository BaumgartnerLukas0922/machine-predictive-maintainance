{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "735da6b6",
   "metadata": {},
   "source": [
    "# Evaluation Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c41e6114",
   "metadata": {},
   "source": [
    "#### Uninteresting stuff we already talked about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f11c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "maintenance = pd.read_csv(\"predictive_maintenance_prepared.csv\")\n",
    "\n",
    "X = maintenance[maintenance.columns[0:-1]]\n",
    "y = maintenance['Target']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=20, test_size=0.3, stratify=y)\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4d2e4ea",
   "metadata": {},
   "source": [
    "## Predicting via RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a0ba630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.23%\n",
      "Recall: 52.94%\n",
      "Precision: 91.53%\n",
      "F1 Score: 67.08%\n",
      "------\n",
      "Ratio\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    6763\n",
       "1     237\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.fit(train_X, train_y)\n",
    "pred_y = rf_model.predict(test_X)\n",
    "accuracy_score = metrics.accuracy_score(pred_y, test_y)\n",
    "\n",
    "print('Accuracy: {:.2%}'.format(metrics.accuracy_score(test_y, pred_y)))\n",
    "print('Recall: {:.2%}'.format(metrics.recall_score(test_y, pred_y)))\n",
    "print('Precision: {:.2%}'.format(metrics.precision_score(test_y, pred_y)))\n",
    "print('F1 Score: {:.2%}'.format(metrics.f1_score(test_y, pred_y)))\n",
    "\n",
    "print('-----')\n",
    "print('Ratio')\n",
    "train_y.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb59cdf8",
   "metadata": {},
   "source": [
    "We can see that even though the Accuracy is quite high, the Recall and F1 score are considerably low. As we work with machines a false negative would be detrimental, so we have to try to get Recall as high as possible. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fefb5b6",
   "metadata": {},
   "source": [
    "Problem is our \"working to not working machines\" ratio is really low ~(96.496% / 3,504%). We have to do a little bit of *magic*. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c477f609",
   "metadata": {},
   "source": [
    "## Random Oversampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f05b4eb",
   "metadata": {},
   "source": [
    "As we could see in the example before, the amount of faulty machines is way smaller than the amount of working machines. To fix this, first we are going to try to oversample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67fd75a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.33%\n",
      "Precision: 86.11%\n",
      "Recall: 60.78%\n",
      "F1: 71.26%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    6763\n",
       "1    6763\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampler = RandomOverSampler(sampling_strategy=1,random_state=20)\n",
    "over_X, over_y = oversampler.fit_resample(train_X, train_y)\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=20)\n",
    "rf_model.fit(over_X, over_y)\n",
    "pred_y = rf_model.predict(test_X)\n",
    "\n",
    "print('Accuracy: {:.2%}'.format(metrics.accuracy_score(test_y, pred_y)))\n",
    "print('Precision: {:.2%}'.format(metrics.precision_score(test_y, pred_y)))\n",
    "print('Recall: {:.2%}'.format(metrics.recall_score(test_y, pred_y)))\n",
    "print('F1: {:.2%}'.format(metrics.f1_score(test_y, pred_y)))\n",
    "\n",
    "over_y.value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f418ff7",
   "metadata": {},
   "source": [
    "The results are way better. Does it really matter? No. Just oversampling or undersampling would of course not be enough so what is the solution?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9c47ae7",
   "metadata": {},
   "source": [
    "## Over- and undersampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b15777eb",
   "metadata": {},
   "source": [
    "Sure, let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f01fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2705\n",
       "1    2705\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampler = RandomOverSampler(sampling_strategy=0.4,random_state=20)\n",
    "ounder_X, ounder_y = oversampler.fit_resample(train_X, train_y)\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy=1, random_state=22)\n",
    "ounder_X, ounder_y = undersampler.fit_resample(ounder_X, ounder_y)\n",
    "\n",
    "ounder_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6ad69c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.80%\n",
      "Precision: 66.07%\n",
      "Recall: 72.55%\n",
      "F1: 69.16%\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(random_state=20)\n",
    "rf_model.fit(ounder_X, ounder_y)\n",
    "y_pred = rf_model.predict(test_X)\n",
    "\n",
    "print('Accuracy: {:.2%}'.format(metrics.accuracy_score(test_y, y_pred)))\n",
    "print('Precision: {:.2%}'.format(metrics.precision_score(test_y, y_pred)))\n",
    "print('Recall: {:.2%}'.format(metrics.recall_score(test_y, y_pred)))\n",
    "print('F1: {:.2%}'.format(metrics.f1_score(test_y, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2032654c",
   "metadata": {},
   "source": [
    "An increase of ~20% to the Recall is very nice for the \"cost\" of just under a 1% in the Accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "63c444f2625e787ea2a4007195cd95f0ab18b1bf75026f8748d6fc196560cdf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
